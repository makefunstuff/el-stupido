# Research Findings: el-stupido

**Date**: 2026-02-23
**Status**: Thesis partially disproven, redirected to more valuable discovery

## Executive Summary

el-stupido started as a "maximally compressed assembly language for LLMs" â€” the
hypothesis being that emoji/short keywords would save tokens and let smaller
models generate code. Through 5 rounds of benchmarking across 12+ models from
776MB to 14.7GB, this thesis was **disproven**: BPE tokenizers already encode
common keywords as single tokens, and emoji actually costs MORE tokens.

However, the project accidentally discovered something more valuable:
**codebook expansion** (tiny declarative specs compiled to large programs)
combined with **constrained decoding** (JSON schema forcing structured output)
makes sub-4B models viable for code generation. This is a genuinely novel
finding with practical applications.

---

## Finding 1: The Original Thesis Is Wrong

**Claim**: Replacing `def`, `for`, `return` with short/emoji tokens (`fn`, `âž°`,
`ret`) saves tokens and makes code generation cheaper for LLMs.

**Reality**: BPE tokenizers (used by all modern LLMs) already encode common
programming keywords as single tokens:

| Token | BPE tokens | el-stupido | BPE tokens |
|-------|-----------|------------|-----------|
| `def` | 1 | `fn` | 1 |
| `for` | 1 | `âž°` | 3-4 |
| `return` | 1 | `ret` | 1 |
| `struct` | 1 | `ðŸ“¦` | 3-4 |
| `while` | 1 | `whl` | 1 |
| `function` | 1 | `ðŸ”§` | 3-4 |

**Emoji is actively harmful**: A single emoji like `ðŸ”§` encodes as 3-4 BPE
tokens (UTF-8 bytes split across subword boundaries). Replacing `function` (1
token) with `ðŸ”§` (3-4 tokens) is a net loss.

Short ASCII keywords (`fn`, `ret`, `whl`) are token-neutral: they're also 1
token each. No savings, no loss. The entire keyword-shortening premise has zero
effect on token count.

## Finding 2: Where Tokens Actually Go

Analysis of a representative 200-token Python program reveals the real token
budget:

| Category | % of tokens | Compressible? |
|----------|------------|--------------|
| Variable names + string literals | 35% | No (irreducible, semantic) |
| Framework boilerplate | 30% | **Yes â€” this is the real waste** |
| Core logic (if/for/math) | 25% | No (irreducible, algorithmic) |
| Structural syntax (braces, colons) | 10% | Barely |

**The enemy is NOT keywords â€” it's framework boilerplate.**

A Flask web server spends ~60 tokens on `from flask import Flask`, `app =
Flask(__name__)`, `@app.route`, `if __name__ == '__main__'`, `app.run()`. None
of this is "the program" â€” it's framework ceremony.

An argparse CLI spends ~40 tokens on `import argparse`, `parser =
ArgumentParser()`, `parser.add_argument(...)` repeated N times,
`args = parser.parse_args()`. Again, boilerplate.

Codebooks eliminate exactly this waste. `use web` + `listen 8080` + routes
replaces all framework ceremony with 3-6 lines.

## Finding 3: Codebook Expansion Ratios

Measured expansion ratios from el-stupido codebook source to compiled output:

| File | Source | Expanded | Ratio | What it builds |
|------|--------|----------|-------|---------------|
| `crud_grugbook.es` | 44B (3 lines) | 2917B | **66x** | Full CRUD web app with HTML forms, .grug persistence |
| `cb_rest_users.es` | 105B (6 lines) | 3449B | **33x** | JSON REST API with model, routes, persistence |
| `cb_test_math.es` | 438B (17 lines) | 1978B | **5x** | Test runner with ANSI colored output |
| `cb_cli_grep.es` | 392B (15 lines) | 1599B | **4x** | CLI tool with flags, args, help text |
| `cb_hello.es` | 81B (4 lines) | 804B | **10x** | HTTP server with routes |

The highest expansion (66x) comes from the most declarative specs. When the LLM
only needs to express intent ("I want a CRUD app for grugbooks with name and
msg fields"), the compiler generates all the structural code.

Key insight: **compression ratio correlates with how declarative the spec is**.
Pure data declarations (model names, field names, route paths) expand the most.
Imperative logic (custom handlers) expands the least.

## Finding 4: Constrained Decoding Is Transformative

Round 5 benchmarked JSON schema constrained decoding vs free-form generation
across 12 models on 5 programming tasks:

**Sub-4B models (9 models tested)**:
- Schema constrained: **9 passes total**
- Free generation: **1 pass total**

Constrained decoding makes sub-4B models 9x more likely to produce correct
code.

**Why it works**: A sub-4B model generating free-form el-stupido source has
~500+ valid next tokens at each step (any keyword, identifier, number, symbol).
With JSON schema constraints, there are typically ~20 valid next tokens. Fewer
decisions per token = fewer chances to go wrong.

**Best results**:
- qwen2.5-coder:7b (schema): 4/5 tasks
- phi4 (schema): 4/5 tasks
- Haiku 3.5 (schema): **5/5 tasks â€” first perfect score**
- qwen3:8b (free): 3/5 tasks (anomalous â€” large enough to learn from prompt)

**Token efficiency**: Schema output averages ~100 tokens vs ~400 for free
generation. 4x more compact.

## Finding 5: The Anomaly at 8B+

qwen3:8b is the only model where free generation (3/5) outperforms schema (2/5).

This reveals a threshold: **above ~8B parameters**, models have enough capacity
to learn a new language's patterns from a prompt alone. Schema constraints
become overhead rather than assistance. Below ~8B, constraints are essential.

This means constrained decoding is specifically a **small model** technique. For
large models, free generation with good prompting works fine.

## Finding 6: Benchmark Progression

| Round | Approach | Pass Rate | Key Change |
|-------|----------|-----------|------------|
| 1 | Original emoji syntax | 2/12 (17%) | Baseline â€” most models can't parse emoji syntax |
| 2 | Natural keywords | 4/12 (33%) | Accept `for`/`while`/`return` as LLMs write them |
| 3 | Declarative combinators | 8/12 (67%) | Eliminate loops from LLM output entirely |
| 4 | HTTP codebook | 6/12 (50%) | 4-line DSL â†’ full server (fewer models, harder task) |
| 5 | JSON schema constrained | 4/5 (80%) | Structured output eliminates syntax errors |
| 5 | Haiku 3.5 | 5/5 (100%) | First perfect score across all tasks |

The trend: **less freedom for the LLM = better results**. Every successful
iteration removed degrees of freedom from the generation space.

## The Refined Thesis

The original thesis â€” "shorter syntax = fewer tokens = better LLM generation" â€”
is wrong.

The refined thesis that the data supports:

> **Structured intermediate representations + compiler expansion outperform
> free-form code generation for small models.**

Specifically:
1. Declarative specs (JSON/codebook) constrain the output space
2. Compiler expansion amplifies minimal specs into large programs
3. Combined, a sub-4B model can produce working applications it could never
   generate as raw code

This is testable, novel, and supported by 5 rounds of benchmarking across 12+
models.

## Implications

### What this means for LLM code generation:

1. **Stop optimizing syntax for LLMs.** Keywords don't matter. Framework
   boilerplate does.

2. **Codebook/template systems are underexplored.** The 66x expansion ratio
   from `crud_grugbook.es` means a model generating 10 tokens of spec produces
   a program that would take 660 tokens to generate directly. This isn't
   compression â€” it's leverage.

3. **Constrained decoding unlocks small models.** Sub-4B models go from nearly
   useless (1 pass out of 45 attempts) to functional (9 passes) with schema
   constraints alone.

4. **The real product is a compilation target, not a language.** El-stupido's
   value isn't as a language humans write â€” it's as an intermediate
   representation that small models can target and compilers can expand.

### What this means for edge/embedded AI:

The logical endpoint: a ~100M parameter model on a Raspberry Pi generating
codebook JSON specs (~20-60 tokens of structured output), fed to the el-stupido
compiler, producing native binaries. A tiny model that can't write code but CAN
fill in a form, and the compiler does the rest.

This is the TinyLLM direction â€” not yet implemented, but the thesis is supported
by the data.

---

## Appendix A: Codebook Examples

### 66x expansion â€” Full CRUD web app (3 lines)
```
use web
listen 8080
crud grugbook name msg+
```
Compiles to: fork-per-connection HTTP server, HTML form generation, URL
decoding, .grug file persistence, list/create/delete handlers. 2917 bytes of C
from 44 bytes of spec.

### 33x expansion â€” JSON REST API (6 lines)
```
use rest
listen 8080
model user name email
GET /users list user
POST /users create user
GET /health "ok"
```
Compiles to: HTTP server, JSON serialization, .grug persistence layer, model
struct, route dispatch, CRUD handlers. 3449 bytes from 105 bytes.

### 10x expansion â€” HTTP server (4 lines)
```
use web
listen 8080
/ "hello from el-stupido!"
/about "emoji-first systems lang"
```
Compiles to: socket/bind/listen/accept/fork, HTTP parsing, route dispatch,
response formatting. 804 bytes from 81 bytes.

## Appendix B: Constrained Decoding Detail

Full results table from Round 5 (P=pass, CF=compile fail, W=wrong, F=fail,
JE=JSON error):

| Model | Size | fact | fizz | sumsq | fib | primes | Schema | Free |
|-------|------|------|------|-------|-----|--------|--------|------|
| deepseek-coder:1.3b | 1B | CF/CF | CF/CF | CF/CF | CF/CF | CF/CF | 0/5 | 0/5 |
| qwen2.5-coder:1.5b | 1.5B | CF/CF | CF/CF | W/CF | CF/CF | CF/CF | 0/5 | 0/5 |
| lfm-opencode:1.2b | 1.2B | CF/CF | CF/CF | CF/CF | CF/CF | CF/CF | 0/5 | 0/5 |
| lfm2.5-thinking:1.2b | 1.2B | P/CF | CF/CF | CF/CF | CF/CF | CF/CF | 1/5 | 0/5 |
| codegemma:2b | 3B | P/CF | W/CF | CF/F | P/CF | CF/CF | 2/5 | 0/5 |
| sam860/LFM2:2.6b | 2.6B | P/CF | CF/CF | CF/CF | CF/CF | JE/CF | 1/5 | 0/5 |
| starcoder2:3b | 3B | P/F | CF/CF | P/F | JE/CF | CF/F | 2/5 | 0/5 |
| qwen2.5-coder:3b | 3.1B | CF/CF | P/CF | CF/CF | P/CF | CF/CF | 2/5 | 0/5 |
| granite4 | 3.4B | CF/P | CF/CF | CF/CF | P/CF | CF/CF | 1/5 | 1/5 |
| qwen2.5-coder:7b | 7.6B | P/CF | P/CF | P/CF | P/CF | CF/CF | 4/5 | 0/5 |
| phi4 | 14.7B | P/CF | P/CF | P/CF | P/CF | CF/CF | 4/5 | 0/5 |
| qwen3:8b | 8.2B | P/P | W/P | CF/CF | P/P | CF/F | 2/5 | 3/5 |

**Aggregate sub-4B**: 9 schema passes vs 1 free pass (9x improvement).
